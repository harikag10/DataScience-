{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Taxi-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning is a type of Machine learning where an agent learns from its environment and takes appropriate actions from a state and earns rewards accordingly. Here we are taking an example environment called Taxi-v3 from the OpenAI gym platform to see how QLearning algorithm, which is one of the RL algorithms, can be used by the agent(Taxi) to achieve its goal. Objective of this game is to pick up a passenger from one of the 4 possible locations and drop him off at another as fast as possible. We aim to achieve the optimized Reinforcement learning algorithm for the agent using QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/gym\")\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '|' represent walls which cannot be crossed and the taxi can pass through ':' to reach the destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_size is  6\n",
      "state_size is  500\n"
     ]
    }
   ],
   "source": [
    "action_size=env.action_space.n\n",
    "state_size=env.observation_space.n\n",
    "print(\"action_size is \",action_size)\n",
    "print(\"state_size is \",state_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing QTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtable=np.zeros((state_size,action_size))\n",
    "qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q learning algorithm - lets train the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, learning_rate, gamma, epsilon, min_epsilon, max_epsilon, decay_rate, max_steps, total_episodes):\n",
    "    # List of rewards \n",
    "    rewards = []\n",
    "    rewards_copy = []\n",
    "    average_rewards = []\n",
    "    steps = []\n",
    "    # Until learning is stopped\n",
    "    for episode in range(total_episodes):\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        step=0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # randomize a number for the exploration-exploitation trade-off\n",
    "            exp_exp_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "            ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "            if exp_exp_tradeoff > epsilon:\n",
    "                action = np.argmax(qtable[state,:])\n",
    "\n",
    "            # Else exploration (random choice of action)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Updating the Qvalue of current state-action pair using the bellman equation\n",
    "            qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "\n",
    "            # Our new state is state\n",
    "            state = new_state\n",
    "\n",
    "            # Update rewards\n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                steps.append(step)\n",
    "                rewards.append(total_rewards)\n",
    "                rewards_copy.append(total_rewards)\n",
    "                #print (\"Score\", total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "\n",
    "        # Reduce epsilon \n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "        \n",
    "        # Finding the average rewards per 100 episodes\n",
    "        if (episode+1) % 100 == 0:\n",
    "                average_reward = np.mean(rewards)\n",
    "                average_rewards.append(average_reward)\n",
    "                rewards = []\n",
    "\n",
    "        if (episode+1) % 100 == 0:    \n",
    "                print('Episode {} Average Reward: {}'.format(episode+1, average_reward))\n",
    "\n",
    "    env.close()\n",
    "    print (\"Score over time: \" +  str(sum(rewards_copy)/total_episodes))\n",
    "    print(\"Average number of steps per episode: \" +str(sum(steps)/total_episodes))\n",
    "    return rewards_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Establish a baseline performance. How well did your RL Q-learning do on your problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha =0.7, gamma=0.8, epsilon=1.0, min_epsilon=0.01, max_epsilon=1.0, decay_rate = 0.01, max_steps=99 and total_episodes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -81.06493506493507\n",
      "Episode 200 Average Reward: -17.15\n",
      "Episode 300 Average Reward: -2.84\n",
      "Episode 400 Average Reward: 2.5\n",
      "Episode 500 Average Reward: 5.21\n",
      "Episode 600 Average Reward: 6.49\n",
      "Episode 700 Average Reward: 6.85\n",
      "Episode 800 Average Reward: 7.38\n",
      "Episode 900 Average Reward: 6.54\n",
      "Episode 1000 Average Reward: 7.1\n",
      "Episode 1100 Average Reward: 7.32\n",
      "Episode 1200 Average Reward: 7.1\n",
      "Episode 1300 Average Reward: 7.38\n",
      "Episode 1400 Average Reward: 7.92\n",
      "Episode 1500 Average Reward: 6.87\n",
      "Episode 1600 Average Reward: 7.45\n",
      "Episode 1700 Average Reward: 7.43\n",
      "Episode 1800 Average Reward: 7.06\n",
      "Episode 1900 Average Reward: 6.54\n",
      "Episode 2000 Average Reward: 7.86\n",
      "Episode 2100 Average Reward: 7.41\n",
      "Episode 2200 Average Reward: 7.69\n",
      "Episode 2300 Average Reward: 6.55\n",
      "Episode 2400 Average Reward: 7.8\n",
      "Episode 2500 Average Reward: 7.67\n",
      "Episode 2600 Average Reward: 7.27\n",
      "Episode 2700 Average Reward: 7.27\n",
      "Episode 2800 Average Reward: 7.35\n",
      "Episode 2900 Average Reward: 7.31\n",
      "Episode 3000 Average Reward: 7.77\n",
      "Episode 3100 Average Reward: 7.17\n",
      "Episode 3200 Average Reward: 6.79\n",
      "Episode 3300 Average Reward: 7.21\n",
      "Episode 3400 Average Reward: 7.7\n",
      "Episode 3500 Average Reward: 7.89\n",
      "Episode 3600 Average Reward: 7.19\n",
      "Episode 3700 Average Reward: 7.27\n",
      "Episode 3800 Average Reward: 7.63\n",
      "Episode 3900 Average Reward: 7.29\n",
      "Episode 4000 Average Reward: 7.76\n",
      "Episode 4100 Average Reward: 7.35\n",
      "Episode 4200 Average Reward: 7.41\n",
      "Episode 4300 Average Reward: 6.9\n",
      "Episode 4400 Average Reward: 8.08\n",
      "Episode 4500 Average Reward: 7.51\n",
      "Episode 4600 Average Reward: 7.27\n",
      "Episode 4700 Average Reward: 7.5\n",
      "Episode 4800 Average Reward: 7.34\n",
      "Episode 4900 Average Reward: 7.37\n",
      "Episode 5000 Average Reward: 7.25\n",
      "Score over time: 5.0912\n",
      "Average number of steps per episode: 13.0654\n"
     ]
    }
   ],
   "source": [
    "# Calling the QLearning agent function  \n",
    "rewards_sum=QLearning(env,0.7,0.8,1.0,0.01,1.0,0.01,99,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the average reward improves as the agent learns through episodes and an overall score is ~5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 7.77\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "# Running for a 100 test episodes\n",
    "for episode in range(100):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    #print(\"****************************************************\")\n",
    "    #print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(99):\n",
    "        # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n",
    "        # env.render()\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            #print (\"Score\", total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing the game for a 100 test episodes results in an overall score 7.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1, len(rewards_sum)+1), rewards_sum)\n",
    "plt.ylabel('rewards_sum')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the sum of rewards per episode shows that in the initial episodes , the rewards were negative and it has gradually increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are the states, the actions and the size of the Q-table?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The state space is 500 as we have 25 different positions for the taxi, 4 different destinations and 5 passenger locations and hence 25*5*4= 500 observations space\n",
    "- And there are 6 different actions \n",
    "1. move south\n",
    "2. move north\n",
    "3. move west\n",
    "4. move east\n",
    "5. pickup\n",
    "6. drop off\n",
    "So the size of qtable is 500*6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(qtable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are the rewards? Why did you choose them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards are as chosen as follows\n",
    "- +20 points for successfull drop-off, as we need a high positive reward for this behaviour as it is highly desired\n",
    "- -10 points penalty for illegal pickup or drop off\n",
    "- And a slight negative reward for each time step as we need the agent to reach the destination as fast as possible, so -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How did you choose alpha and gamma in the following equation?\n",
    "Try at least one additional value for alpha and gamma. How did it change the baseline performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- alpha is the learning rate, values ranging from 0-1. It has to gradually decrease as the number of iterations increase and agents learns \n",
    "- gamma is the discount factor, which determines the importance given to the future rewards. It ranges from 0-1\n",
    "Lets try with following values and see its impact on the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -77.02702702702703\n",
      "Episode 200 Average Reward: -9.44\n",
      "Episode 300 Average Reward: 1.12\n",
      "Episode 400 Average Reward: 4.83\n",
      "Episode 500 Average Reward: 6.0\n",
      "Episode 600 Average Reward: 7.04\n",
      "Episode 700 Average Reward: 7.92\n",
      "Episode 800 Average Reward: 7.56\n",
      "Episode 900 Average Reward: 6.97\n",
      "Episode 1000 Average Reward: 7.73\n",
      "Episode 1100 Average Reward: 7.15\n",
      "Episode 1200 Average Reward: 6.92\n",
      "Episode 1300 Average Reward: 7.06\n",
      "Episode 1400 Average Reward: 7.91\n",
      "Episode 1500 Average Reward: 7.86\n",
      "Episode 1600 Average Reward: 7.54\n",
      "Episode 1700 Average Reward: 8.05\n",
      "Episode 1800 Average Reward: 7.96\n",
      "Episode 1900 Average Reward: 7.23\n",
      "Episode 2000 Average Reward: 7.12\n",
      "Episode 2100 Average Reward: 7.25\n",
      "Episode 2200 Average Reward: 7.26\n",
      "Episode 2300 Average Reward: 8.26\n",
      "Episode 2400 Average Reward: 7.06\n",
      "Episode 2500 Average Reward: 7.81\n",
      "Episode 2600 Average Reward: 7.69\n",
      "Episode 2700 Average Reward: 7.0\n",
      "Episode 2800 Average Reward: 7.75\n",
      "Episode 2900 Average Reward: 7.87\n",
      "Episode 3000 Average Reward: 6.82\n",
      "Episode 3100 Average Reward: 7.79\n",
      "Episode 3200 Average Reward: 7.88\n",
      "Episode 3300 Average Reward: 7.7\n",
      "Episode 3400 Average Reward: 7.27\n",
      "Episode 3500 Average Reward: 8.06\n",
      "Episode 3600 Average Reward: 7.17\n",
      "Episode 3700 Average Reward: 7.59\n",
      "Episode 3800 Average Reward: 6.88\n",
      "Episode 3900 Average Reward: 8.1\n",
      "Episode 4000 Average Reward: 7.4\n",
      "Episode 4100 Average Reward: 6.81\n",
      "Episode 4200 Average Reward: 7.43\n",
      "Episode 4300 Average Reward: 7.5\n",
      "Episode 4400 Average Reward: 7.34\n",
      "Episode 4500 Average Reward: 7.75\n",
      "Episode 4600 Average Reward: 7.21\n",
      "Episode 4700 Average Reward: 7.58\n",
      "Episode 4800 Average Reward: 7.49\n",
      "Episode 4900 Average Reward: 7.57\n",
      "Episode 5000 Average Reward: 6.61\n",
      "Score over time: 5.6286\n",
      "Average number of steps per episode: 12.7248\n"
     ]
    }
   ],
   "source": [
    "# new values for alpha and gamma \n",
    "#gamma = 0.9\n",
    "#learning_rate(alpha) = 0.3\n",
    "\n",
    "#QLearning(env, alpha, gamma, epsilon, min_epsilon, max_epsilon, decay_rate, max_steps, total_episodes)\n",
    "rewards_sum=QLearning(env,0.5,0.9,1.0,0.01,1.0,0.01,99,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can infer that the score has improved by reducing the learning rate and increased the gamma as the agent has learnt many times and gradually its performance improves so we can reduce the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Score has improved from 5.1 to 5.6, i.e a 9.8% increase from the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try a policy other than maxQ(s', a'). How did it change the baseline performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to take minimum instead of max qvalue policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning_new(env, learning_rate, gamma, epsilon, min_epsilon, max_epsilon, decay_rate, max_steps, total_episodes):\n",
    "    # List of rewards \n",
    "    rewards = []\n",
    "    rewards_copy = []\n",
    "    average_rewards = []\n",
    "    steps = []\n",
    "    std_dev = np.std(qtable,axis=1)\n",
    "    # Until learning is stopped\n",
    "    for episode in range(total_episodes):\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # randomize a number for the exploration-exploitation trade-off\n",
    "            exp_exp_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "            ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "            if exp_exp_tradeoff > epsilon:\n",
    "                action = np.argmax(qtable[state,:])\n",
    "\n",
    "            # Else exploration (random choice of action)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Updating the Qvalue of current state-action pair using the bellman equation\n",
    "            qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.min(qtable[new_state, :]) - qtable[state, action])\n",
    "\n",
    "            # Our new state is state\n",
    "            state = new_state\n",
    "\n",
    "            # Update rewards\n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                steps.append(step)\n",
    "                rewards.append(total_rewards)\n",
    "                rewards_copy.append(total_rewards)\n",
    "                #print (\"Score\", total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "\n",
    "        # Reduce epsilon \n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "\n",
    "        if (episode+1) % 100 == 0:\n",
    "                average_reward = np.mean(rewards)\n",
    "                average_rewards.append(average_reward)\n",
    "                rewards = []\n",
    "\n",
    "        if (episode+1) % 100 == 0:    \n",
    "                print('Episode {} Average Reward: {}'.format(episode+1, average_reward))\n",
    "\n",
    "    env.close()\n",
    "    print (\"Score over time: \" +  str(sum(rewards_copy)/total_episodes))\n",
    "    print(\"Average number of steps per episode: \" +str(sum(steps)/total_episodes) )\n",
    "    return rewards_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -150.14285714285714\n",
      "Episode 200 Average Reward: -81.11111111111111\n",
      "Episode 300 Average Reward: -58.827586206896555\n",
      "Episode 400 Average Reward: -25.444444444444443\n",
      "Episode 500 Average Reward: -7.25\n",
      "Episode 600 Average Reward: -9.26086956521739\n",
      "Episode 700 Average Reward: -6.611111111111111\n",
      "Episode 800 Average Reward: -7.862068965517241\n",
      "Episode 900 Average Reward: -2.4857142857142858\n",
      "Episode 1000 Average Reward: 5.0\n",
      "Episode 1100 Average Reward: 0.7586206896551724\n",
      "Episode 1200 Average Reward: -2.6666666666666665\n",
      "Episode 1300 Average Reward: -2.0526315789473686\n",
      "Episode 1400 Average Reward: -0.9473684210526315\n",
      "Episode 1500 Average Reward: -1.619047619047619\n",
      "Episode 1600 Average Reward: 5.5476190476190474\n",
      "Episode 1700 Average Reward: 3.7346938775510203\n",
      "Episode 1800 Average Reward: 5.28\n",
      "Episode 1900 Average Reward: 3.5172413793103448\n",
      "Episode 2000 Average Reward: -2.489795918367347\n",
      "Episode 2100 Average Reward: 4.9655172413793105\n",
      "Episode 2200 Average Reward: 0.0392156862745098\n",
      "Episode 2300 Average Reward: 0.4375\n",
      "Episode 2400 Average Reward: 1.0\n",
      "Episode 2500 Average Reward: -10.035714285714286\n",
      "Episode 2600 Average Reward: -1.3225806451612903\n",
      "Episode 2700 Average Reward: 2.380952380952381\n",
      "Episode 2800 Average Reward: 0.723404255319149\n",
      "Episode 2900 Average Reward: 0.7659574468085106\n",
      "Episode 3000 Average Reward: 0.48936170212765956\n",
      "Episode 3100 Average Reward: 3.0851063829787235\n",
      "Episode 3200 Average Reward: 0.21568627450980393\n",
      "Episode 3300 Average Reward: 4.65\n",
      "Episode 3400 Average Reward: -1.2777777777777777\n",
      "Episode 3500 Average Reward: -0.49122807017543857\n",
      "Episode 3600 Average Reward: 0.5925925925925926\n",
      "Episode 3700 Average Reward: 4.888888888888889\n",
      "Episode 3800 Average Reward: 3.744186046511628\n",
      "Episode 3900 Average Reward: 2.3157894736842106\n",
      "Episode 4000 Average Reward: 2.036363636363636\n",
      "Episode 4100 Average Reward: 3.4310344827586206\n",
      "Episode 4200 Average Reward: 5.22\n",
      "Episode 4300 Average Reward: 3.8\n",
      "Episode 4400 Average Reward: 5.774193548387097\n",
      "Episode 4500 Average Reward: 3.938775510204082\n",
      "Episode 4600 Average Reward: 2.6615384615384614\n",
      "Episode 4700 Average Reward: 0.47619047619047616\n",
      "Episode 4800 Average Reward: 4.245901639344262\n",
      "Episode 4900 Average Reward: 4.983333333333333\n",
      "Episode 5000 Average Reward: 0.3333333333333333\n",
      "Score over time: -0.3662\n",
      "Average number of steps per episode: 8.1774\n"
     ]
    }
   ],
   "source": [
    "#QLearning_new(env, alpha, gamma, epsilon, min_epsilon, max_epsilon, decay_rate, max_steps, total_episodes) for trying a policy other than maxQvalue\n",
    "rewards_sum_new=QLearning_new(env,0.7,0.8,1.0,0.01,1.0,0.01,99,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is clear that by taking the min policy, we have not achieved good results. The average rewards are not imporving consistently and the score has also reduced drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of maxargs, lets try to take the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_dev = np.std(qtable,axis=1)\n",
    "np.shape(std_dev)\n",
    "#x=np.min(np.std(qtable,axis=1))\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning_new(env, learning_rate, gamma, epsilon, min_epsilon, max_epsilon, decay_rate, max_steps, total_episodes):\n",
    "    # List of rewards \n",
    "    rewards = []\n",
    "    rewards_copy = []\n",
    "    average_rewards = []\n",
    "    steps = []\n",
    "    std_dev = np.std(qtable,axis=1)\n",
    "    # Until learning is stopped\n",
    "    for episode in range(total_episodes):\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # randomize a number for the exploration-exploitation trade-off\n",
    "            exp_exp_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "            ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "            if exp_exp_tradeoff > epsilon:\n",
    "                action = np.argmax(qtable[state,:])\n",
    "\n",
    "            # Else exploration (random choice of action)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Updating the Qvalue of current state-action pair using the bellman equation\n",
    "            qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.mean(qtable[new_state, :]) - qtable[state, action])\n",
    "\n",
    "            # Our new state is state\n",
    "            state = new_state\n",
    "\n",
    "            # Update rewards\n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                steps.append(step)\n",
    "                rewards.append(total_rewards)\n",
    "                rewards_copy.append(total_rewards)\n",
    "                #print (\"Score\", total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "\n",
    "        # Reduce epsilon \n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "\n",
    "        if (episode+1) % 100 == 0:\n",
    "                average_reward = np.mean(rewards)\n",
    "                average_rewards.append(average_reward)\n",
    "                rewards = []\n",
    "\n",
    "        if (episode+1) % 100 == 0:    \n",
    "                print('Episode {} Average Reward: {}'.format(episode+1, average_reward))\n",
    "\n",
    "    env.close()\n",
    "    print (\"Score over time: \" +  str(sum(rewards_copy)/total_episodes))\n",
    "    print(\"Average number of steps per episode: \" +str(sum(steps)/total_episodes) )\n",
    "    return rewards_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -114.67857142857143\n",
      "Episode 200 Average Reward: -59.36842105263158\n",
      "Episode 300 Average Reward: -22.916666666666668\n",
      "Episode 400 Average Reward: -4.714285714285714\n",
      "Episode 500 Average Reward: -6.5\n",
      "Episode 600 Average Reward: -6.888888888888889\n",
      "Episode 700 Average Reward: 2.857142857142857\n",
      "Episode 800 Average Reward: 9.0\n",
      "Episode 900 Average Reward: 2.3333333333333335\n",
      "Episode 1000 Average Reward: -0.6666666666666666\n",
      "Episode 1100 Average Reward: -2.1\n",
      "Episode 1200 Average Reward: -5.75\n",
      "Episode 1300 Average Reward: -7.333333333333333\n",
      "Episode 1400 Average Reward: -4.4\n",
      "Episode 1500 Average Reward: -2.75\n",
      "Episode 1600 Average Reward: 4.2727272727272725\n",
      "Episode 1700 Average Reward: -1.0\n",
      "Episode 1800 Average Reward: -4.666666666666667\n",
      "Episode 1900 Average Reward: -6.666666666666667\n",
      "Episode 2000 Average Reward: 1.3333333333333333\n",
      "Episode 2100 Average Reward: 5.714285714285714\n",
      "Episode 2200 Average Reward: 10.5\n",
      "Episode 2300 Average Reward: -8.714285714285714\n",
      "Episode 2400 Average Reward: -5.25\n",
      "Episode 2500 Average Reward: 1.5\n",
      "Episode 2600 Average Reward: 0.5\n",
      "Episode 2700 Average Reward: 0.3333333333333333\n",
      "Episode 2800 Average Reward: 12.0\n",
      "Episode 2900 Average Reward: -9.5\n",
      "Episode 3000 Average Reward: -9.666666666666666\n",
      "Episode 3100 Average Reward: -7.5\n",
      "Episode 3200 Average Reward: 1.5\n",
      "Episode 3300 Average Reward: 9.571428571428571\n",
      "Episode 3400 Average Reward: -1.3333333333333333\n",
      "Episode 3500 Average Reward: 10.428571428571429\n",
      "Episode 3600 Average Reward: -7.0\n",
      "Episode 3700 Average Reward: 4.875\n",
      "Episode 3800 Average Reward: 9.0\n",
      "Episode 3900 Average Reward: 7.4\n",
      "Episode 4000 Average Reward: -13.25\n",
      "Episode 4100 Average Reward: -1.4285714285714286\n",
      "Episode 4200 Average Reward: -3.727272727272727\n",
      "Episode 4300 Average Reward: 8.5\n",
      "Episode 4400 Average Reward: 10.0\n",
      "Episode 4500 Average Reward: -4.8\n",
      "Episode 4600 Average Reward: 6.5\n",
      "Episode 4700 Average Reward: 1.8\n",
      "Episode 4800 Average Reward: -5.2\n",
      "Episode 4900 Average Reward: 8.5\n",
      "Episode 5000 Average Reward: 8.375\n",
      "Score over time: -0.9098\n",
      "Average number of steps per episode: 1.7318\n"
     ]
    }
   ],
   "source": [
    "#QLearning_new(env, alpha, gamma, epsilon, min_epsilon, max_epsilon, decay_rate, max_steps, total_episodes) for trying a policy other than maxQvalue\n",
    "rewards_sum_new=QLearning_new(env,0.7,0.8,1.0,0.01,1.0,0.01,99,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How did you choose your decay rate and starting epsilon? Try at least one additional value for epsilon and the decay rate. How did it change the baseline performance? What is the value of epsilon when if you reach the max steps per episode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initally as the agent has to explore the environment, have chosen the epsilon to be 1.0 and as the agent starts exploring and learning, so that qvalues are updated, we can then exploit.\n",
    "We choose a decay rate to decay the epsilon value to achieve the exploration-exploitation trade off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -81.11111111111111\n",
      "Episode 200 Average Reward: -54.0\n",
      "Episode 300 Average Reward: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harika/Applications/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/harika/Applications/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400 Average Reward: nan\n",
      "Episode 500 Average Reward: nan\n",
      "Episode 600 Average Reward: nan\n",
      "Episode 700 Average Reward: -0.3333333333333333\n",
      "Episode 800 Average Reward: -7.5\n",
      "Episode 900 Average Reward: -5.2\n",
      "Episode 1000 Average Reward: nan\n",
      "Episode 1100 Average Reward: 10.0\n",
      "Episode 1200 Average Reward: 8.75\n",
      "Episode 1300 Average Reward: 10.5\n",
      "Episode 1400 Average Reward: 12.0\n",
      "Episode 1500 Average Reward: -3.3333333333333335\n",
      "Episode 1600 Average Reward: 8.0\n",
      "Episode 1700 Average Reward: 6.5\n",
      "Episode 1800 Average Reward: 9.25\n",
      "Episode 1900 Average Reward: -6.4\n",
      "Episode 2000 Average Reward: 8.333333333333334\n",
      "Episode 2100 Average Reward: 12.0\n",
      "Episode 2200 Average Reward: 5.4\n",
      "Episode 2300 Average Reward: -10.333333333333334\n",
      "Episode 2400 Average Reward: 7.714285714285714\n",
      "Episode 2500 Average Reward: 7.571428571428571\n",
      "Episode 2600 Average Reward: 7.0\n",
      "Episode 2700 Average Reward: 6.2\n",
      "Episode 2800 Average Reward: 9.5\n",
      "Episode 2900 Average Reward: 5.857142857142857\n",
      "Episode 3000 Average Reward: 6.0\n",
      "Episode 3100 Average Reward: 10.25\n",
      "Episode 3200 Average Reward: 10.0\n",
      "Episode 3300 Average Reward: -17.666666666666668\n",
      "Episode 3400 Average Reward: 10.0\n",
      "Episode 3500 Average Reward: 8.714285714285714\n",
      "Episode 3600 Average Reward: -7.5\n",
      "Episode 3700 Average Reward: 8.75\n",
      "Episode 3800 Average Reward: -29.5\n",
      "Episode 3900 Average Reward: 8.0\n",
      "Episode 4000 Average Reward: 7.75\n",
      "Episode 4100 Average Reward: 6.285714285714286\n",
      "Episode 4200 Average Reward: 7.5\n",
      "Episode 4300 Average Reward: 7.333333333333333\n",
      "Episode 4400 Average Reward: 6.25\n",
      "Episode 4500 Average Reward: 7.857142857142857\n",
      "Episode 4600 Average Reward: 4.818181818181818\n",
      "Episode 4700 Average Reward: 3.875\n",
      "Episode 4800 Average Reward: 9.0\n",
      "Episode 4900 Average Reward: 8.25\n",
      "Episode 5000 Average Reward: 6.285714285714286\n",
      "Score over time: 0.0378\n",
      "Average number of steps per episode: 0.6738\n"
     ]
    }
   ],
   "source": [
    "# Trying out the following values for epsilon and decay rate on top of baseline model\n",
    "# epsilon = 0.5, decay rate =0.05\n",
    "#QLearning(env, alpha, gamma, epsilon, min_epsilon, max_epsilon, decay_rate, max_steps, total_episodes)\n",
    "rewards_sum=QLearning(env,0.7,0.8,0.5,0.01,1.0,0.04,99,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by reducing the epsilon, we have reduced our score overall as we have not allowed for good exploration for the agent initially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the average number of steps taken per episode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -100.87096774193549\n",
      "Episode 200 Average Reward: -40.1764705882353\n",
      "Episode 300 Average Reward: -11.114285714285714\n",
      "Episode 400 Average Reward: 2.64\n",
      "Episode 500 Average Reward: 2.0277777777777777\n",
      "Episode 600 Average Reward: 4.297297297297297\n",
      "Episode 700 Average Reward: 8.882352941176471\n",
      "Episode 800 Average Reward: 6.45\n",
      "Episode 900 Average Reward: 7.923076923076923\n",
      "Episode 1000 Average Reward: 8.428571428571429\n",
      "Episode 1100 Average Reward: 4.972222222222222\n",
      "Episode 1200 Average Reward: 7.764705882352941\n",
      "Episode 1300 Average Reward: 6.956521739130435\n",
      "Episode 1400 Average Reward: 8.186046511627907\n",
      "Episode 1500 Average Reward: 7.551020408163265\n",
      "Episode 1600 Average Reward: 7.5813953488372094\n",
      "Episode 1700 Average Reward: 7.5\n",
      "Episode 1800 Average Reward: 8.89655172413793\n",
      "Episode 1900 Average Reward: 7.666666666666667\n",
      "Episode 2000 Average Reward: 7.394736842105263\n",
      "Episode 2100 Average Reward: 8.745098039215685\n",
      "Episode 2200 Average Reward: 7.659090909090909\n",
      "Episode 2300 Average Reward: 8.054054054054054\n",
      "Episode 2400 Average Reward: 6.342105263157895\n",
      "Episode 2500 Average Reward: 8.340909090909092\n",
      "Episode 2600 Average Reward: 8.688888888888888\n",
      "Episode 2700 Average Reward: 8.647058823529411\n",
      "Episode 2800 Average Reward: 6.864864864864865\n",
      "Episode 2900 Average Reward: 8.057142857142857\n",
      "Episode 3000 Average Reward: 5.285714285714286\n",
      "Episode 3100 Average Reward: 7.783783783783784\n",
      "Episode 3200 Average Reward: 7.8\n",
      "Episode 3300 Average Reward: 8.181818181818182\n",
      "Episode 3400 Average Reward: 7.469387755102041\n",
      "Episode 3500 Average Reward: 8.394736842105264\n",
      "Episode 3600 Average Reward: 8.166666666666666\n",
      "Episode 3700 Average Reward: 7.088888888888889\n",
      "Episode 3800 Average Reward: 8.466666666666667\n",
      "Episode 3900 Average Reward: 7.205128205128205\n",
      "Episode 4000 Average Reward: 7.166666666666667\n",
      "Episode 4100 Average Reward: 8.23076923076923\n",
      "Episode 4200 Average Reward: 7.88\n",
      "Episode 4300 Average Reward: 7.357142857142857\n",
      "Episode 4400 Average Reward: 8.666666666666666\n",
      "Episode 4500 Average Reward: 7.733333333333333\n",
      "Episode 4600 Average Reward: 7.28\n",
      "Episode 4700 Average Reward: 7.826086956521739\n",
      "Episode 4800 Average Reward: 7.026315789473684\n",
      "Episode 4900 Average Reward: 7.978260869565218\n",
      "Episode 5000 Average Reward: 8.372093023255815\n",
      "Score over time: 1.8932\n",
      "Average number of steps per episode: 5.434\n"
     ]
    }
   ],
   "source": [
    "# Calculating based on the baseline model\n",
    "rewards_sum=QLearning(env,0.7,0.8,1.0,0.01,1.0,0.01,99,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on an average the agent takes approximately 5.4 steps per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Does Q-learning use value-based or policy-based iteration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q learning uses value based iteration as here our goal is to find the optimal state value function, i.e we try to find the approximate optimum q value function. The next action from a given state is chosen based on the values from the qtable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is meant by expected lifetime value in the Bellman equation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman equation expresses the relationship between value of a state and values of its successor states, showing that value of a state under a policy pi, is the sum of its expected return(discounted value of the next state) plus the rewards along its way. \n",
    "So the expected lifetime value is the path that can be taken from an intial state following the maximum state-action value pairs(to get the maximum expected return), once an opotimal policy is reached and there after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"\Bellman.png\" alt=\"title\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used Qlearning algorithm to determine the best path to be taken by a taxi to pick up a passenger and drop-off at one of the destinations as fast as possible.\n",
    "- We tuned the parameters, alpha(Learning rate), gamma(discount factor), epsilon, decay rate and the policy used to alter the agent's performance.\n",
    "- In this particular game, we have used the score to evaluate the agent's performance over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Harika Reddy Gurram\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
